{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# GLM Analysis (Measured)\n\nIn this example we analyse data from a real multichannel\nfunctional near-infrared spectroscopy (fNIRS)\nexperiment (see `tut-fnirs-hrf-sim` for a simplified simulated\nanalysis). The experiment consists of three conditions\n1) tapping with the left hand,\n2) tapping with the right hand,\n3) a control condition where the participant does nothing.\nWe use a GLM analysis to examine the neural activity associated with\nthe different tapping conditions.\nAn alternative epoching style analysis on the same data can be\nviewed in the\n`waveform analysis example <tut-fnirs-processing>`.\nSee\n`Luke et al (2021) <https://www.spiedigitallibrary.org/journals/neurophotonics/volume-8/issue-2/025008/Analysis-methods-for-measuring-passive-auditory-fNIRS-responses-generated-by/10.1117/1.NPh.8.2.025008.short>`_\nfor a comparison of the epoching and GLM approaches.\n\nThis GLM analysis is a wrapper over the excellent\n`Nilearn GLM <http://nilearn.github.io/modules/reference.html#module-nilearn.glm>`_.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 8\n\n# Authors: Robert Luke <mail@robertluke.net>\n#\n# License: BSD (3-clause)\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport mne\nimport mne_nirs\n\nfrom mne_nirs.experimental_design import make_first_level_design_matrix\nfrom mne_nirs.statistics import run_GLM, glm_region_of_interest\nfrom mne_nirs.visualisation import plot_glm_topo\nfrom mne_nirs.channels import (get_long_channels, get_short_channels,\n                               picks_pair_to_idx)\n\nfrom nilearn.plotting import plot_design_matrix\nfrom mne_nirs.utils._io import glm_to_tidy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import raw NIRS data\n\nFirst we import the motor tapping data, these data are also\ndescribed and used in the\n`MNE fNIRS tutorial <mne:tut-fnirs-processing>`\n\nAfter reading the data we resample down to 1Hz\nto meet github memory constraints.\n\n.. collapse:: Data description (click to expand)\n   :class: success\n\n   Optodes were placed over the motor cortex using the standard NIRX motor\n   montage, but with 8 short channels added (see their web page for details).\n   To view the sensor locations run\n   `raw_intensity.plot_sensors()`.\n   A sound was presented to indicate which hand the participant should tap.\n   Participants tapped their thumb to their fingers for 5s.\n   Conditions were presented in a random order with a randomised inter\n   stimulus interval.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fnirs_data_folder = mne.datasets.fnirs_motor.data_path()\nfnirs_raw_dir = os.path.join(fnirs_data_folder, 'Participant-1')\nraw_intensity = mne.io.read_raw_nirx(fnirs_raw_dir).load_data()\nraw_intensity.resample(0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean up annotations before analysis\n\nNext we update the annotations by assigning names to each trigger ID.\nThen we crop the recording to the section containing our\nexperimental conditions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "original_annotations = raw_intensity.annotations\nnew_des = [des for des in raw_intensity.annotations.description]\nnew_des = ['Control' if x == \"1.0\" else x for x in new_des]\nnew_des = ['Tapping/Left' if x == \"2.0\" else x for x in new_des]\nnew_des = ['Tapping/Right' if x == \"3.0\" else x for x in new_des]\nkeepers = [n == 'Control' or\n           n == \"Tapping/Left\" or\n           n == \"Tapping/Right\" for n in new_des]\nidxs = np.array(np.where(keepers)[0])\nannot = mne.Annotations(original_annotations.onset[idxs],\n                        original_annotations.duration[idxs] * 5., \n                        np.array([new_des[idx] for idx in np.where(keepers)[0]]))\nraw_intensity.set_annotations(annot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess NIRS data\nNext we convert the raw data to haemoglobin concentration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)\nraw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. sidebar:: Relevant literature\n\n   Tachtsidis, Ilias, and Felix Scholkmann. \"False positives and false\n   negatives in functional near-infrared spectroscopy: issues, challenges,\n   and the way forward.\" Neurophotonics 3.3 (2016): 031405.\n\nWe then split the data in to\nshort channels which predominantly contain systemic responses and\nlong channels which have both neural and systemic contributions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "short_chs = get_short_channels(raw_haemo)\nraw_haemo = get_long_channels(raw_haemo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View experiment events\n\nNext we examine the timing and order of events in this experiment.\nThere are several options for how to view event information.\nThe first option is to use MNE's plot events command.\nHere each dot represents when an event started.\nWe observe that the order of conditions was randomised and the time between\nevents is also randomised.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "events, _ = mne.events_from_annotations(raw_haemo, verbose=False)\nevent_dict = {'Control': 1, 'Tapping/Left': 2, 'Tapping/Right': 3}\nmne.viz.plot_events(events, event_id=event_dict,\n                    sfreq=raw_haemo.info['sfreq'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The previous plot did not illustrate the duration that an event lasted for.\nAlternatively, we can view the experiment using a boxcar plot, where the\nline is raised for the duration of the stimulus/condition.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s = mne_nirs.experimental_design.create_boxcar(raw_haemo)\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15, 6))\nplt.plot(raw_haemo.times, s, axes=axes)\nplt.legend([\"Control\", \"Left\", \"Right\"], loc=\"upper right\")\nplt.xlabel(\"Time (s)\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create design matrix\n\n.. sidebar:: Relevant literature\n\n   For further discussion on design matrices see\n   the Nilearn examples. Specifically the \n   `first level model <http://nilearn.github.io/auto_examples/04_glm_first_level/plot_first_level_details.html>`_\n   and \n   `design matrix examples <http://nilearn.github.io/auto_examples/04_glm_first_level/plot_design_matrix.html>`_.\n\nNext we create a model to fit our data to.\nThe model consists of various components to model different things we assume\ncontribute to the measured signal.\nWe model the expected neural response for each experimental condition\nusing the SPM haemodynamic response\nfunction (HRF) combined with the known stimulus event times and durations\n(as described above).\nWe also include a third order polynomial drift and constant to model\nslow fluctuations in the data and a constant DC shift.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "design_matrix = make_first_level_design_matrix(raw_haemo,\n                                               hrf_model='spm', stim_dur=5.0,\n                                               drift_order=3,\n                                               drift_model='polynomial')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also add the mean of the short channels to the design matrix.\nIn theory these channels contain only systemic components, so including\nthem in the design matrix allows us to estimate the neural component\nrelated to each experimental condition\nuncontaminated by systemic effects.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "design_matrix[\"ShortHbO\"] = np.mean(short_chs.copy().pick(\n                                    picks=\"hbo\").get_data(), axis=0)\n\ndesign_matrix[\"ShortHbR\"] = np.mean(short_chs.copy().pick(\n                                    picks=\"hbr\").get_data(), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we display a summary of the design matrix\nusing standard Nilearn reporting functions.\nThe first three columns represent the SPM HRF convolved with our stimulus\nevent information.\nThe next columns illustrate the drift and constant components.\nThe last columns illustrate the short channel signals.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax1 = plt.subplots(figsize=(10, 6), nrows=1, ncols=1)\nfig = plot_design_matrix(design_matrix, ax=ax1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Examine expected response\n\nThe matrices above can be a bit abstract as they encompase multiple \nconditions and regressors.\nInstead we can examine a single condition.\nHere we observe the boxcar function for a single condition,\nthis illustrates when the stimulus was active.\nWe also view the expected neural response using the HRF specified above,\nwe observe that each time a stimulus is presented there is an expected\nbrain response that lags the stimulus onset and consists of a large positive\ncomponent followed by an undershoot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s = mne_nirs.experimental_design.create_boxcar(raw_intensity)\nplt.plot(raw_intensity.times, s[:, 1])\nplt.plot(design_matrix['Tapping/Left'])\nplt.xlim(180, 300)\nplt.legend([\"Stimulus\", \"Expected Response\"])\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Amplitude\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit GLM to subset of data and estimate response for each experimental condition\n\n.. sidebar:: Relevant literature\n\n   Huppert TJ. Commentary on the statistical properties of noise and its\n   implication on general linear models in functional near-infrared\n   spectroscopy. Neurophotonics. 2016;3(1)\n\nWe run a GLM fit for the data and experiment matrix.\nFirst we analyse just the first two channels which correspond to HbO and HbR\nof a single source detector pair.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_subset = raw_haemo.copy().pick(picks=range(2))\nglm_est = run_GLM(data_subset, design_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This returns a GLM regression estimate for each channel.\nThis data is stored in a dictionary, which can be addressed using the\ndesired channel name. You can view an overview of the estimates as:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "glm_est"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or you can view the estimate for a single channel by indexing it by name.\nYou can see that for each channel a standard\n`Nilearn RegressionResults object <https://nilearn.github.io/modules/generated/nilearn.glm.RegressionResults.html>`_\nis returned. These objects are rich with information that can be requested\nfrom the object, for example to determine the mean square error of the\nestimate you would call\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "glm_est['S1_D1 hbo'].MSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Due to the richness of the object we provide a function `glm_to_tidy` to\nextract commonly used information and put it in a convenient dataframe/table.\nBelow this is demonstrated and then we just display the first 9 rows of the\ntable which correspond to the 9 components of the design matrix for the\nfirst channel.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "glm_to_tidy(data_subset, glm_est, design_matrix).head(9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then display the results. In this example we address the objects directly.\nNote that the control condition sits\naround zero\nand that the HbO is positive and larger than the HbR, this is to be expected.\nFurther, we note that for this channel the response to tapping on the\nright hand is larger than the left. And the values are similar to what\nis seen in the epoching tutorial.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.scatter(design_matrix.columns[:3], glm_est['S1_D1 hbo'].theta[:3] * 1e6)\nplt.scatter(design_matrix.columns[:3], glm_est['S1_D1 hbr'].theta[:3] * 1e6)\nplt.xlabel(\"Experiment Condition\")\nplt.ylabel(\"Haemoglobin (\u03bcM)\")\nplt.legend([\"Oxyhaemoglobin\", \"Deoxyhaemoglobin\"])\nplt.hlines([0.0], 0, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit GLM to all data and view topographic distribution\n\nLastly we can run the GLM analysis on all sensors and plot the result on a\ntopomap.\nWe see the same result as in the MNE tutorial,\nthat activation is largest\ncontralateral to the tapping side. Also note that HbR tends to be the\nnegative of HbO as expected.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "glm_est = run_GLM(raw_haemo, design_matrix)\nplot_glm_topo(raw_haemo, glm_est, design_matrix,\n              requested_conditions=['Tapping/Left',\n                                    'Tapping/Right'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the topographic visualisation is a high level representation\nof the underlying data. This visual representation fits a smoothed surface\nto the data and makes many assumptions including that the data is\nspatially smooth and that the sensors sufficiently cover the scalp surface.\nThese assumptions can be violated with fNIRS due to the improved spatial\nsensitivity (relative to EEG) and typically low number of sensors that are\nunevenly distributed over the scalp.\nAs such, researchers should understand the underlying data and ensure that\nthe figure accurately reflects the effect of interest.\n\nAs an example of how the topoplot can be deceiving, we replot\nthe `Tapping/Right` condition from above for each hemisphere\nseparately. When both hemisphere are plotted together (left),\nthe function smooths\nthe large space between sensors, making the activity on the left hemisphere\nsmear towards the center and appear larger than the underlying data shows.\nWhen each hemisphere is plotted independently (right) it becomes immediately\napparent that the data does not indicate that activity spreads across\nthe center of the head.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 6),\n                         gridspec_kw=dict(width_ratios=[0.92, 1]))\n\nplot_glm_topo(raw_haemo.copy().pick(picks=\"hbo\"), glm_est, design_matrix,\n              axes=axes[0], colorbar=False,\n              requested_conditions=['Tapping/Right'])\nplot_glm_topo(raw_haemo.copy().pick(picks=\"hbo\").pick(picks=range(10)),\n              glm_est, design_matrix, vmin=-16, vmax=16, axes=axes[1],\n              requested_conditions=['Tapping/Right'], colorbar=False)\nplot_glm_topo(raw_haemo.copy().pick(picks=\"hbo\").pick(picks=range(10, 20)),\n              glm_est, design_matrix, axes=axes[1], vmin=-16, vmax=16,\n              requested_conditions=['Tapping/Right'])\n\naxes[0].set_title(\"Smoothed across hemispheres\")\naxes[1].set_title(\"Hemispheres plotted independently\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyse regions of interest\n\n.. sidebar:: Relevant literature\n\n   Zimeo Morais, G.A., Balardin, J.B. & Sato, J.R.\n   fNIRS Optodes\u2019 Location Decider (fOLD): a toolbox for probe arrangement\n   guided by brain regions-of-interest. Sci Rep 8, 3341 (2018).\n\n   Shader and Luke et al. \"The use of broad vs restricted regions of\n   interest in functional near-infrared spectroscopy for measuring cortical\n   activation to auditory-only and visual-only speech.\"\n   Hearing Research (2021): `108256 <https://www.sciencedirect.com/science/article/pii/S0378595521000903>`_.\n\nOr alternatively we can summarise the responses across regions of interest\nfor each condition. And you can plot it with your favorite software.\nRegion of interest analysis can be more robust than single channel analysis.\nThe fOLD toolbox can be used to assist in the design of ROIs.\nAnd consideration should be paid to ensure optimal size ROIs are selected.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "left = [[1, 1], [1, 2], [1, 3], [2, 1], [2, 3],\n        [2, 4], [3, 2], [3, 3], [4, 3], [4, 4]]\nright = [[5, 5], [5, 6], [5, 7], [6, 5], [6, 7],\n         [6, 8], [7, 6], [7, 7], [8, 7], [8, 8]]\n\ngroups = dict(Left_ROI=picks_pair_to_idx(raw_haemo, left),\n              Right_ROI=picks_pair_to_idx(raw_haemo, right))\n\ndf = pd.DataFrame()\nfor idx, col in enumerate(design_matrix.columns[:3]):\n    df = df.append(glm_region_of_interest(glm_est, groups, idx, col))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As with the single channel results above, this is placed in a tidy dataframe\nwhich contains conveniently extracted information, but now for the region\nof interest.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute contrasts\n\nWe can also define a contrast as described in\n`Nilearn docs <http://nilearn.github.io/auto_examples/04_glm_first_level/plot_localizer_surface_analysis.html>`_\nand plot it.\nHere we contrast the response to tapping on the left hand with the response\nfrom tapping on the right hand.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "contrast_matrix = np.eye(design_matrix.shape[1])\nbasic_conts = dict([(column, contrast_matrix[i])\n                   for i, column in enumerate(design_matrix.columns)])\ncontrast_LvR = basic_conts['Tapping/Left'] - basic_conts['Tapping/Right']\ncontrast = mne_nirs.statistics.compute_contrast(glm_est, contrast_LvR)\nmne_nirs.visualisation.plot_glm_contrast_topo(raw_haemo, contrast)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Results\n\n.. sidebar:: Relevant literature\n\n   Wickham, Hadley. \"Tidy data.\" Journal of Statistical Software 59.10 (2014): 1-23.\n\nHere we export the data in a tidy pandas data frame.\nWe export the GLM results for every channel and condition.\nData is exported in long format by default.\nHowever, a helper function is also provided to convert the long data to wide format.\nThe long to wide conversion also adds some additional derived data, such as\nif a significant response (p<0.05) was observed, which sensor and detector is\nin the channel, which chroma, etc.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = glm_to_tidy(raw_haemo, glm_est, design_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Determine true and false positive rates\n\nWe can query the exported data frames to determine the true and false\npositive rates. Note: optodes cover a greater region than just the\nmotor cortex, so we dont expect 100% of channels to detect responses to\nthe tapping, but we do expect 5% or less for the false positive rate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(df\n .query('Condition in [\"Control\", \"Tapping/Left\", \"Tapping/Right\"]')\n .groupby(['Condition', 'Chroma'])\n .agg(['mean'])\n .drop(['df', 'mse', 'p_value', 't'], 1)\n )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}